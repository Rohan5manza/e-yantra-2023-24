# -*- coding: utf-8 -*-
"""e_yantra (7).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10Z6WNa_MBpd49fWrKtMGwI79IPRvcIOg
"""

import torch
import pandas as pd
from sklearn.preprocessing import OneHotEncoder  # or try labelencoder if this doesnt work
import numpy as np
import torch
from torch.utils.data import TensorDataset
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from torch.utils.data import DataLoader
from sklearn import datasets
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

dataframe= pd.read_csv('task_1a_dataset.csv')

dataframe

def data_preprocessing(dataframe):


  dataframe=dataframe.dropna()

  columns_to_encode=['Education','City','Gender','EverBenched']

  encoder=OneHotEncoder(sparse=False,drop='first')

  encoded_features=encoder.fit_transform(dataframe[columns_to_encode])

  encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(columns_to_encode))

  encoded=dataframe.drop(columns=columns_to_encode).join(encoded_df)

  return encoded


encoded_dataframe=data_preprocessing(dataframe)
print(encoded_dataframe)

encoded_dataframe

def identify_features_and_targets(encoded_dataframe):
    # Drop the target column to get the features
    features = encoded_dataframe.drop('LeaveOrNot', axis=1)
    target_label = 'LeaveOrNot'

    # Create a DataFrame with a single row containing the features and target label as columns
    result = pd.DataFrame({'Features': [features.columns.tolist()], 'Target': [target_label]})
    return result

# Assuming encoded_dataframe is your encoded DataFrame
# You can call the function and get the features and target label as a DataFrame
features_and_targets_df = identify_features_and_targets(encoded_dataframe)

print(features_and_targets_df)

features_and_targets=identify_features_and_targets(encoded_dataframe)
features_and_targets

import torch
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        x_sample = self.X[idx]
        y_sample = self.y[idx]
        return x_sample, y_sample



import torch
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

# Assume CustomDataset is defined appropriately

def load_as_tensors(features_and_targets_df):
    # Extract features and target label from the DataFrame
    features = encoded_dataframe.drop('LeaveOrNot', axis=1)
    target_label = encoded_dataframe['LeaveOrNot']

    # Assuming your data is in the form of a DataFrame
    # Split the data into training and testing sets
    x_train, x_test, y_train, y_test = train_test_split(features.values, target_label.values, test_size=0.2, random_state=30)

    # Convert to PyTorch tensors
    X_train_tensor = torch.Tensor(x_train)
    X_test_tensor = torch.Tensor(x_test)
    y_train_tensor = torch.Tensor(y_train)
    y_test_tensor = torch.Tensor(y_test)

    # Create custom dataset instances for training and testing
    train_dataset = CustomDataset(X_train_tensor, y_train_tensor)
    test_dataset = CustomDataset(X_test_tensor, y_test_tensor)

    # Create a DataLoader for training data
    batch_size = 64  # Adjust as needed
    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)

    return [X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, train_dataloader]

tensors_and_iterable_training_data = load_as_tensors(features_and_targets)
tensors_and_iterable_training_data

from torch.utils.data import DataLoader
X_train_tensor, X_test_tensor, y_train_tensor,y_test_tensor,train_dataloader = load_as_tensors(features_and_targets)
print("X_train_tensor shape:", X_train_tensor.shape)
print("y_train_tensor shape:", y_train_tensor.shape)
print("X_test_tensor shape:", X_test_tensor.shape)
print("y_test_tensor shape:", y_test_tensor.shape)
print("train_dataloader:",train_dataloader)

class Salary_Predictor(nn.Module):
    def __init__(self,input_size, hidden_size, output_size):
        super(Salary_Predictor, self).__init__()
        # Define your model architecture here
        self.fc1 = torch.nn.Linear(input_size,hidden_size)
        self.fc2 = torch.nn.Linear(hidden_size,output_size)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x


input_size = X_train_tensor.shape[1]  # Automatically adjust based on the dataset
hidden_size = 128  # Adjust based on your dataset and constraints
output_size = 1

model=Salary_Predictor(input_size,hidden_size,output_size)

def model_loss_function(): #can be in-built from pytorch, use binary cross entropy
  return nn.BCELoss()

loss_function=model_loss_function()

import torch.optim as optim
def model_optimizer(model):
  return torch.optim.Adam(model.parameters(), lr=0.01)

optimizer=model_optimizer(model)

def model_number_of_epochs():
  return 10

number_of_epochs=model_number_of_epochs()

def training_function(model, number_of_epochs, tensors_and_iterable_training_data, loss_function, optimizer):

	# Unpack tensors and iterable training data
	X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, train_loader = tensors_and_iterable_training_data;y_train_tensor = y_train_tensor.view(-1, 1)   ;y_test_tensor = y_test_tensor.view(-1, 1)

	for epoch in range(number_of_epochs):
		model.train()  # Set the model in training mode

		# Initialize variables to track loss and accuracy
		total_loss = 0.0
		correct_predictions = 0

		for batch_x, batch_y in train_loader:
			# Zero the gradients
			optimizer.zero_grad()

			# Forward pass
			outputs = model(batch_x);batch_y=batch_y.view(-1,1)
			# Calculate loss
			loss = loss_function(outputs,batch_y)

			# Backpropagate and update parameters
			loss.backward()
			optimizer.step()

			# Accumulate loss
			total_loss += loss.item()

			# Calculate the number of correct predictions
			_, predicted = torch.max(outputs, 1)
			correct_predictions += (predicted == batch_y).sum().item()

		# Calculate average loss and accuracy for the epoch
		avg_loss = total_loss / len(train_loader.dataset)
		accuracy = correct_predictions / len(train_loader.dataset)

		# # Print training progress

	return model

def validation_function(trained_model, tensors_and_iterable_training_data):

	 # Unpack tensors and validation data loader
	X_train_tensor, X_test_tensor, y_train_tensor, y_test_tensor, test_loader = tensors_and_iterable_training_data

	trained_model.eval()  # Set the model in evaluation mode

	correct_predictions = 0
	total_examples = 0

	with torch.no_grad():  # Disable gradient computation for validation
		for batch_x, batch_y in test_loader:
			# Forward pass to make predictions
			outputs = trained_model(batch_x)


			# Calculate the number of correct predictions
			_, predicted = torch.max(outputs, 1)
			correct_predictions += (predicted == batch_y).sum().item()
			total_examples += batch_y.size(0)

	# Calculate the accuracy
	model_accuracy = correct_predictions / total_examples


	return model_accuracy

if __name__ == "__main__":

    # reading the provided dataset csv file using pandas library and
    # converting it to a pandas Dataframe
    task_1a_dataframe = pd.read_csv('task_1a_dataset.csv')


    # data preprocessing and obtaining encoded data
    encoded_dataframe = data_preprocessing(task_1a_dataframe)

    # selecting required features and targets
    features_and_targets = identify_features_and_targets(encoded_dataframe)


    tensors_and_iterable_training_data = load_as_tensors(features_and_targets)
    # model is an instance of the class that defines the architecture of the model
    model = Salary_Predictor(input_size,hidden_size,output_size)

    # obtaining loss function, optimizer and the number of training epochs
    loss_function = model_loss_function()
    optimizer = model_optimizer(model)
    number_of_epochs = model_number_of_epochs()

    # training the model

    trained_model = training_function(model, number_of_epochs, tensors_and_iterable_training_data,
                    loss_function, optimizer)

    # validating and obtaining accuracy
    model_accuracy = validation_function(trained_model,tensors_and_iterable_training_data)
    print(f"Accuracy on the test set = {model_accuracy}")

    X_train_tensor = tensors_and_iterable_training_data[0]
    x = X_train_tensor[0]
    jitted_model = torch.jit.save(torch.jit.trace(model, (x)), "task_1a_trained_model.pth")